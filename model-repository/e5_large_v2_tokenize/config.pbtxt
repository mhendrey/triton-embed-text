name: "e5_large_v2_tokenize"
backend: "python"
max_batch_size: 32
default_model_filename: "e5_large_v2_tokenize.py"

input [
    {
        name: "INPUT_TEXT"
        data_type: TYPE_STRING
        dims: [1]
    }
]
output [
    {
        name: "INPUT_IDS"
        data_type: TYPE_INT64
        dims: [512]
    },
    {
        name: "ATTENTION_MASK"
        data_type: TYPE_INT64
        dims: [512]
    }
]

parameters: [
    {
        key: "EXECUTION_ENV_PATH",
        value: {string_value: "$$TRITON_MODEL_DIRECTORY/e5_large_v2_tokenize.tar.gz"},
    },
    {
        key: "default_truncation",
        value: {string_value: "false"},
    }
]
instance_group [
    {
        kind: KIND_CPU,
        count: 1
    }
]
version_policy: {latest: {num_versions: 1}}
dynamic_batching: {}